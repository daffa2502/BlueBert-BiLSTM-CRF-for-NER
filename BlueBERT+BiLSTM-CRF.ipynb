{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install torchcrf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification, EvalPrediction, TrainerCallback, BertModel, BertTokenizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "from TorchCRF import CRF\n",
        "\n",
        "train_df = pd.read_csv('/kaggle/input/biobert/train.csv')\n",
        "dev_df = pd.read_csv('/kaggle/input/biobert/dev.csv')\n",
        "test_df = pd.read_csv('/kaggle/input/biobert/test.csv')\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, label_map, max_len):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_map = label_map\n",
        "        self.max_len = max_len\n",
        "        self.sentences, self.labels = self._split_sentences_and_labels(dataframe)\n",
        "\n",
        "    def _split_sentences_and_labels(self, dataframe):\n",
        "        sentences = []\n",
        "        labels = []\n",
        "        sentence = []\n",
        "        label = []\n",
        "        for idx in range(len(dataframe)):\n",
        "            word = dataframe.iloc[idx, 0]\n",
        "            tag = dataframe.iloc[idx, 1]\n",
        "            if pd.isna(word) or word == '':\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    labels.append(label)\n",
        "                    sentence = []\n",
        "                    label = []\n",
        "            else:\n",
        "                sentence.append(word)\n",
        "                label.append(tag)\n",
        "        if sentence:\n",
        "            sentences.append(sentence)\n",
        "            labels.append(label)\n",
        "        return sentences, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence = self.sentences[index]\n",
        "        tags = self.labels[index]\n",
        "\n",
        "        tokens = []\n",
        "        label_ids = []\n",
        "\n",
        "        for word, label in zip(sentence, tags):\n",
        "            word_tokens = self.tokenizer.tokenize(word)\n",
        "            tokens.extend(word_tokens)\n",
        "            label_ids.extend([self.label_map[label]] * len(word_tokens))\n",
        "\n",
        "        tokens = tokens[:self.max_len - 2]\n",
        "        label_ids = label_ids[:self.max_len - 2]\n",
        "\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "        label_ids = [33] + label_ids + [33]\n",
        "\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        padding_length = self.max_len - len(input_ids)\n",
        "        input_ids = input_ids + ([0] * padding_length)\n",
        "        attention_mask = attention_mask + ([0] * padding_length)\n",
        "        label_ids = label_ids + ([33] * padding_length)\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
        "            'labels': torch.tensor(label_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "MAX_LEN = 204\n",
        "train_dataset = NERDataset(train_df, tokenizer, label_map, MAX_LEN)\n",
        "dev_dataset = NERDataset(dev_df, tokenizer, label_map, MAX_LEN)\n",
        "test_dataset = NERDataset(test_df, tokenizer, label_map, MAX_LEN)\n",
        "\n",
        "class BertBiLSTMCRF(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_labels, hidden_dim=768, lstm_dim=256, class_weights=None):\n",
        "        super(BertBiLSTMCRF, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.lstm = nn.LSTM(hidden_dim, lstm_dim, batch_first=True, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(lstm_dim * 2, num_labels)\n",
        "        self.crf = CRF(num_labels)\n",
        "        self.num_labels = num_labels\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "\n",
        "        lstm_out, _ = self.lstm(sequence_output)\n",
        "        emissions = self.hidden2tag(lstm_out)\n",
        "\n",
        "        if self.class_weights is not None:\n",
        "            weight_tensor = self.class_weights.clone().detach()\n",
        "            weight_tensor = weight_tensor.to(emissions.device)\n",
        "            weight_tensor = weight_tensor.view(1, 1, -1)\n",
        "            weighted_emissions = emissions * weight_tensor\n",
        "            emissions = weighted_emissions\n",
        "\n",
        "        if labels is not None:\n",
        "            log_likelihood = self.crf(emissions, labels, mask=attention_mask.byte())\n",
        "            loss = -log_likelihood.mean()\n",
        "            return loss, emissions\n",
        "        else:\n",
        "            return emissions\n",
        "\n",
        "    def predict(self, input_ids, attention_mask=None):\n",
        "        emissions = self.forward(input_ids, attention_mask)\n",
        "        prediction = self.crf.viterbi_decode(emissions, mask=attention_mask.byte())\n",
        "        return prediction\n",
        "\n",
        "def compute_train_accuracy(trainer, dataloader):\n",
        "    trainer.model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        inputs = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            emissions = trainer.model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "            preds = trainer.model.crf.viterbi_decode(emissions, mask=inputs['attention_mask'].byte())\n",
        "\n",
        "        labels = inputs['labels'].cpu().numpy()\n",
        "        attention_mask = inputs['attention_mask'].cpu().numpy()\n",
        "\n",
        "        for i in range(labels.shape[0]):\n",
        "            label_seq = labels[i]\n",
        "            pred_seq = preds[i]\n",
        "            mask_seq = attention_mask[i]\n",
        "\n",
        "            active_labels = label_seq[mask_seq == 1]\n",
        "            active_preds = pred_seq[:len(active_labels)]\n",
        "\n",
        "            all_labels.extend(active_labels)\n",
        "            all_preds.extend(active_preds)\n",
        "\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_preds = np.array(all_preds)\n",
        "\n",
        "    active_indices = all_labels != 33\n",
        "\n",
        "    all_labels = all_labels[active_indices]\n",
        "    all_preds = all_preds[active_indices]\n",
        "\n",
        "    return accuracy_score(all_labels, all_preds)\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    predictions, labels = p.predictions, p.label_ids\n",
        "\n",
        "    true_labels = labels.flatten().tolist()\n",
        "    pred_labels = np.argmax(predictions, axis=2).flatten().tolist()\n",
        "\n",
        "    active_indices = labels.flatten() != 33\n",
        "    true_labels = [label for label, active in zip(true_labels, active_indices) if active]\n",
        "    pred_labels = [label for label, active in zip(pred_labels, active_indices) if active]\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "class LoggingCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.train_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.eval_losses = []\n",
        "        self.eval_accuracies = []\n",
        "\n",
        "    def on_epoch_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        logs = state.log_history[-1]\n",
        "        epoch = logs.get(\"epoch\", \"N/A\")\n",
        "        train_loss = logs.get(\"loss\", \"N/A\")\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=args.per_device_train_batch_size, collate_fn=data_collator)\n",
        "        train_accuracy = compute_train_accuracy(trainer, train_dataloader)\n",
        "\n",
        "        validation_metrics = trainer.evaluate(test_dataset)\n",
        "        validation_loss = validation_metrics['eval_loss']\n",
        "        validation_accuracy = validation_metrics['eval_accuracy']\n",
        "\n",
        "        self.train_losses.append(train_loss)\n",
        "        self.train_accuracies.append(train_accuracy)\n",
        "        self.eval_losses.append(validation_loss)\n",
        "        self.eval_accuracies.append(validation_accuracy)\n",
        "\n",
        "        print(f\"Epoch: {epoch}\")\n",
        "        print(f\"Training Loss: {train_loss}\")\n",
        "        print(f\"Training Accuracy: {train_accuracy}\")\n",
        "        print(f\"Validation Loss: {validation_loss}\")\n",
        "        print(f\"Validation Accuracy: {validation_accuracy}\")\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        epochs = range(1, len(self.train_losses) + 1)\n",
        "\n",
        "        plt.figure(figsize=(14, 6))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(epochs, self.train_losses, 'b', label='Training loss')\n",
        "        plt.plot(epochs, self.eval_losses, 'r', label='Validation loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs, self.train_accuracies, 'b', label='Training accuracy')\n",
        "        plt.plot(epochs, self.eval_accuracies, 'r', label='Validation accuracy')\n",
        "        plt.title('Training and Validation Accuracy')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "class_supports = [103, 38, 2197, 3827, 476, 22, 8886, 145, 803, 286, 1195, 61, 247, 166, 2693, 609, 24, 17, 1333, 2582, 140, 2, 5463, 7, 336, 27, 439, 8, 42, 82, 1660, 310, 74915]\n",
        "total_samples = sum(class_supports)\n",
        "num_classes = len(class_supports)\n",
        "\n",
        "manual_class_weights = []\n",
        "for i, support in enumerate(class_supports):\n",
        "    if i == 33:\n",
        "        continue\n",
        "    weight = total_samples / (num_classes * support)\n",
        "    manual_class_weights.append(weight)\n",
        "\n",
        "manual_class_weights.insert(33, 0.0001)\n",
        "\n",
        "print(\"Manual Class Weights:\", manual_class_weights)\n",
        "\n",
        "manual_class_weights = torch.tensor(manual_class_weights, dtype=torch.float)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "manual_class_weights = manual_class_weights.to(device)\n",
        "\n",
        "model = BertBiLSTMCRF(\"bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12\", num_labels=34, class_weights=manual_class_weights)\n",
        "model.to(device)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs= 150,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"no\"\n",
        ")\n",
        "\n",
        "logging_callback = LoggingCallback()\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[logging_callback]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "logging_callback.plot_metrics()\n",
        "\n",
        "def evaluate_on_test(trainer, test_dataset):\n",
        "    predictions, labels, _ = trainer.predict(test_dataset)\n",
        "\n",
        "    true_labels = labels.flatten().tolist()\n",
        "    pred_labels = np.argmax(predictions, axis=2).flatten().tolist()\n",
        "\n",
        "    active_indices = labels.flatten() != 33\n",
        "    true_labels = [label for label, active in zip(true_labels, active_indices) if active]\n",
        "    pred_labels = [label for label, active in zip(pred_labels, active_indices) if active]\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='macro')\n",
        "    report = classification_report(true_labels, pred_labels, zero_division=0)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1}\")\n",
        "    print(report)\n",
        "\n",
        "evaluate_on_test(trainer, test_dataset)\n",
        "evaluate_on_test(trainer, train_dataset)\n",
        "\n",
        "def predict_and_get_labels(trainer, dataset):\n",
        "    predictions, labels, _ = trainer.predict(dataset)\n",
        "\n",
        "    true_labels = labels.flatten().tolist()\n",
        "    pred_labels = np.argmax(predictions, axis=2).flatten().tolist()\n",
        "\n",
        "    active_indices = labels.flatten() != 33  # Assuming 33 is the padding label\n",
        "    true_labels = [label for label, active in zip(true_labels, active_indices) if active]\n",
        "    pred_labels = [label for label, active in zip(pred_labels, active_indices) if active]\n",
        "\n",
        "    return true_labels, pred_labels\n",
        "\n",
        "true_labels, pred_labels = predict_and_get_labels(trainer, test_dataset)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# Optionally, compute other metrics\n",
        "report = classification_report(true_labels, pred_labels, zero_division=0)\n",
        "print(report)\n",
        "\n",
        "# Display the confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(20, 20))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
        "disp.plot(ax=ax, cmap='Blues')\n",
        "plt.savefig(\"confmat tes3\")\n",
        "plt.show()\n",
        "\n",
        "save_path = './bert_bilstm_crf_model.pth'\n",
        "\n",
        "# Save the model's state dictionary\n",
        "torch.save(model.state_dict(), save_path)\n",
        "\n",
        "print(f\"Model saved to {save_path}\")"
      ],
      "metadata": {
        "id": "LsgyRaBl0C7E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}